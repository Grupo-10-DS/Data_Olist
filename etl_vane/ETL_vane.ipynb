{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Load\n",
    "\n",
    "path = \"./data/e-comerce_Olist_dataset\"\n",
    "load = Load(path)\n",
    "\n",
    "# Cargamos todos los csv a un diccionario que alojara los df\n",
    "data_dict = load.load_from_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "import os  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traemos aqui los csv\n",
    "clientes = data_dict['olist_customers_dataset']\n",
    "geolocalizacion = data_dict['olist_geolocation_dataset']\n",
    "items = data_dict['olist_order_items_dataset']\n",
    "metodo_de_pago = data_dict['olist_order_payments_dataset']\n",
    "reviews = data_dict['olist_order_reviews_dataset']\n",
    "ordenes =data_dict['olist_orders_dataset']\n",
    "productos = data_dict['olist_products_dataset']\n",
    "vendedores = data_dict['olist_sellers_dataset']\n",
    "productos_info = data_dict['product_category_name_translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las carpetas para guardar nuestros archivos etl y auxiliar que usaremos mas adelante\n",
    "path_etl = \"Dataset_etl\"\n",
    "path_aux = \"Dataset_aux\"\n",
    "os.makedirs(path_etl, exist_ok=True)  \n",
    "os.makedirs(path_aux, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Gelolocalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_geolocalizacion(geolocalizacion):\n",
    "    \"\"\"\n",
    "    Esta funcion le realiza ETL a la tabla geolocalizacion, normalizando latitudes y longitudes tomando como parametro\n",
    "    que la mayor parte de brasil se halla dentro del hemisferio sur, entre los paralelos 5.5 de latitud N, y los -34 de latitud S; y entre los meridianos que seÃ±alan los -32 y los -74 de longitud.\n",
    "    En un segundo paso crea un nuevo dataframe con zip codes unicos y se les asigna un ID.\n",
    "    retorna el dataframe de geolocalizacion al que solo se le realiza el ETL, y el df filtrado para ser utilizados luego en el proceso de etl de vendedores y clientes\n",
    "    \n",
    "    \"\"\"\n",
    "    # Seleccionamos los outliers. \n",
    "    out_latylon = geolocalizacion [(geolocalizacion.geolocation_lat > 5.5)| (geolocalizacion.geolocation_lat < -34)|(geolocalizacion.geolocation_lng > -32 )| (geolocalizacion.geolocation_lng  < -74)]\n",
    "\n",
    "    # generamos un user random\n",
    "    import string, random\n",
    "    user = ''.join(random.choice(string.ascii_lowercase) for i in range(6))\n",
    "\n",
    "    #Usamos Geopy\n",
    "    from geopy.geocoders import Nominatim\n",
    "    geolocator = Nominatim(user_agent= user)\n",
    "    location_city = out_latylon.geolocation_city.values\n",
    "    index = out_latylon.index\n",
    "    for i in range(0,len(location_city)):\n",
    "        location = geolocator.geocode(location_city[i]+', Brasil')\n",
    "        try:\n",
    "            geolocalizacion.loc[geolocalizacion.index==index[i], 'geolocation_lng'] = location.longitude\n",
    "            geolocalizacion.loc[geolocalizacion.index==index[i], 'geolocation_lat'] = location.latitude\n",
    "        except:\n",
    "            try:\n",
    "                location = geolocator.geocode(location_city[i])\n",
    "                geolocalizacion.loc[geolocalizacion.index==index[i], 'geolocation_lng'] = location.longitude\n",
    "                geolocalizacion.loc[geolocalizacion.index==index[i], 'geolocation_lat'] = location.latitude\n",
    "            except:\n",
    "                geolocalizacion.loc[geolocalizacion.index==index[i], 'geolocation_lat'] = geolocalizacion[geolocalizacion.geolocation_city==location_city[i]].geolocation_lat.mean()\n",
    "                geolocalizacion.loc[geolocalizacion.index==index[i], 'geolocation_lng'] = geolocalizacion[geolocalizacion.geolocation_city==location_city[i]].geolocation_lng.mean()\n",
    "\n",
    "    # ahora no es realmente necesario exportar el corregido.. pero si quisieramos podemos ejecutar el codigo:\n",
    "    geolocalizacion.to_csv('data\\e-comerce_Olist_dataset\\olist_geolocation_dataset_coregido.csv', index_label=False)\n",
    "\n",
    "    # Agrupamos por geolocation_zip_code_prefix\n",
    "    geoloc_filtrado = geolocalizacion.groupby('geolocation_zip_code_prefix').mean().reset_index()\n",
    "    # agregamos columna de Id igual a la de index\n",
    "    geoloc_filtrado['IdGeolocalizacion'] = geoloc_filtrado.index\n",
    "\n",
    "    # Agregamos Ciudad y Estado\n",
    "    geoloc_filtrado['Ciudad'] = geoloc_filtrado.apply(lambda r: geolocalizacion[geolocalizacion.geolocation_zip_code_prefix==r.geolocation_zip_code_prefix].geolocation_city.values[0], axis =1)\n",
    "    geoloc_filtrado['Estado'] = geoloc_filtrado.apply(lambda r: geolocalizacion[geolocalizacion.geolocation_zip_code_prefix==r.geolocation_zip_code_prefix].geolocation_state.values[0], axis =1)\n",
    "\n",
    "    # renombramos y reordenamos las columnas\n",
    "    geoloc_filtrado.rename(columns={'geolocation_zip_code_prefix': 'zip_code_prefix', \n",
    "                                    'geolocation_lat': 'Latitud',\n",
    "                                    'geolocation_lng': 'Longitud',\n",
    "                                    'IdGeolocalizacion':'Id_Geolocalizacion'}, inplace=True)\n",
    "    geoloc_filtrado = geoloc_filtrado.reindex(columns=['Id_Geolocalizacion', 'zip_code_prefix', 'Latitud', \n",
    "                                    'Longitud','Ciudad','Estado'])\n",
    "\n",
    "\n",
    "    # En la carpeta Dataset_etl guardamos el archivos de geolocalizacion luego del proceso de ETL\n",
    "    geoloc_filtrado.to_csv(\"{}/geolocalizacion_etl.csv\".format(path_etl), index=False)\n",
    "\n",
    "    #Retornamos el df de geolocalizacion, de la primer parte del etl, porque es el que necesitamos para el etl de clientes y vendedores\n",
    "    return geolocalizacion, geoloc_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocalizacion,  geoloc_filtrado = etl_geolocalizacion(geolocalizacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL a clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clientes_etl(clientes):\n",
    "    \"\"\"\n",
    "    Funcion que realiza el ETL a la tabla clientes. \n",
    "        Se normalizan los codigos postales que no coinciden con los de la tabla geolocalizacion\n",
    "        Para los zip codes de clientes que no se encuentran en geolocalizacion pero si se reconoce su ciudad\n",
    "        se reemplazara el zip_code por la moda del zip_code correspondiente a dicha ciudad en geolocalizacion.\n",
    "        Como resultado guardara dos archivos \n",
    "            clientes_etl.csv con el resultado del etl \n",
    "            clientes_aux.csv con los datos que no logro resolver\n",
    "    \"\"\"\n",
    "    # usamos el df de geolocalizacion porque no tiene filtrados los zip_codes y por ende hay mas datos para calcular el modo\n",
    "    geoloc_unique_code = geolocalizacion.geolocation_zip_code_prefix.unique()\n",
    "    clientes_unique_code = clientes.customer_zip_code_prefix.unique()\n",
    "    zip_codes_missing = []\n",
    "    for item in clientes_unique_code:\n",
    "        if item not in geoloc_unique_code:\n",
    "            try:\n",
    "                city = clientes.loc[clientes['customer_zip_code_prefix'] == item].customer_city.values[0]\n",
    "                zip_code = geolocalizacion[geolocalizacion.geolocation_city==city].geolocation_zip_code_prefix.mode()[0]\n",
    "                clientes['customer_zip_code_prefix'] = clientes['customer_zip_code_prefix'].replace(item, zip_code)\n",
    "            except:\n",
    "                zip_codes_missing.append(item)\n",
    "    \n",
    "    # buscamos los index de los codigos que no logramos resolver para luego guardarlos en una tabla auxiliar\n",
    "    indexes = []\n",
    "    for item in zip_codes_missing:\n",
    "        ind = clientes[clientes.customer_zip_code_prefix==item].index.values\n",
    "        for i in ind:\n",
    "            indexes.append(i)\n",
    "\n",
    "    cientes_aux = clientes.iloc[indexes]\n",
    "    clientes.drop(index = indexes, axis=0, inplace=True)\n",
    "\n",
    "    # Ahora que eliminamos estos valores problema aplicamos el id_Geolocalizacion a la tabla clientes\n",
    "    clientes['Id_Geolocalizacion'] = clientes.apply(lambda r: geoloc_filtrado[geoloc_filtrado.zip_code_prefix==r.customer_zip_code_prefix].Id_Geolocalizacion.values[0], axis = 1)\n",
    "    clientes = clientes.drop(['customer_zip_code_prefix','customer_city','customer_state'],axis=1)\n",
    "\n",
    "    # guardamos clientes_aux en un csv\n",
    "    cientes_aux.to_csv(\"{}/cientes_aux.csv\".format(path_aux), index=False)\n",
    "\n",
    "    # Guardamos las etl\n",
    "    clientes.to_csv(\"{}/clientes_etl.csv\".format(path_etl), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_etl(clientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Vendedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vendedores_etl(vendedores):\n",
    "    \"\"\"\n",
    "    Funcion que realiza el ETL a la tabla vendedores. \n",
    "        Se normalizan los codigos postales que no coinciden con los de la tabla geolocalizacion\n",
    "        Para los zip codes de vendedores que no se encuentran en geolocalizacion pero si se reconoce su ciudad\n",
    "        se reemplazara el zip_code por la moda del zip_code correspondiente a dicha ciudad en geolocalizacion.\n",
    "        Como resultado guardara un archivo vendedores_etl.csv con el resultado del etl \n",
    "    \"\"\"\n",
    "\n",
    "    # Hay 7 codigos postales de vendedores que no coinciden con los codigos de la tabla geolocalizacion.\n",
    "    # Para cada ciudad en la tabla de geolocalizacion hay mas de un zip-code... \n",
    "    # lo que vamos a hacer es cambiar los de los vendedores por el modo de los de geolocalizacion  \n",
    "    # usamos el df de geolocalizacion porque no tiene filtrados los zip_codes y por ende hay mas datos para calcular el modo\n",
    " \n",
    "    geoloc_unique_code = geolocalizacion.geolocation_zip_code_prefix.unique()\n",
    "    vendedores_unique_code = vendedores.seller_zip_code_prefix.unique()\n",
    "    for item in vendedores_unique_code:\n",
    "        if item not in geoloc_unique_code:\n",
    "            city = vendedores.loc[vendedores['seller_zip_code_prefix'] == item].seller_city.values[0]\n",
    "            zip_code = geolocalizacion[geolocalizacion.geolocation_city==city].geolocation_zip_code_prefix.mode()[0]\n",
    "            vendedores['seller_zip_code_prefix'] = vendedores['seller_zip_code_prefix'].replace(item, zip_code)\n",
    "    \n",
    "    #Logramos arreglar todos :) Ahora aplicamos el id_Geolocalizacion a la tabla vendedores\n",
    "\n",
    "    vendedores['Id_Geolocalizacion'] = vendedores.apply(lambda r: geoloc_filtrado[geoloc_filtrado.zip_code_prefix==r.seller_zip_code_prefix].Id_Geolocalizacion.values[0], axis = 1)\n",
    "    vendedores = vendedores.drop(['seller_zip_code_prefix','seller_city','seller_state'],axis=1)\n",
    "\n",
    "    \n",
    "    # Guardamos las etl\n",
    "    vendedores.to_csv(\"{}/vendedores_etl.csv\".format(path_etl), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendedores_etl(vendedores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo de pago // tipo de pago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodo_tipo_pago(metodo_de_pago):\n",
    "    \"\"\"\n",
    "    Se agregan los payment_type_id y se crean las tablas normalizadas payment_type y metodo_de_pago_corregido a partir de la tabla metodo_de_pago\n",
    "    \"\"\"\n",
    "    payment_type = metodo_de_pago['payment_type']\n",
    "    tipos_pago = {j:i+1 for i,j in enumerate(payment_type.unique())}\n",
    "    payment_type_df = pd.DataFrame(\n",
    "        {\n",
    "            'payment_type_id': tipos_pago.values(),\n",
    "            'payment_type': tipos_pago.keys()\n",
    "        })\n",
    "    metodo_de_pago['payment_type'] = metodo_de_pago['payment_type'].apply(lambda x: tipos_pago[x])\n",
    "    metodo_de_pago.rename(columns={'payment_type': 'payment_type_id'}, inplace=True)\n",
    "\n",
    "    #exportamos a csv \n",
    "    payment_type_df.to_csv('{}/payment_type.csv'.format(path_etl), index=False)\n",
    "    metodo_de_pago.to_csv('{}/metodo_de_pago_corregido.csv'.format(path_etl), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metodo_tipo_pago(metodo_de_pago)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordenes // estado de la orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordenes_estado_orden (ordenes):\n",
    "    \"\"\"\n",
    "    Se agregan los order_status_id y se crean las tablas normalizadas order_status y ordenes_corregido a partir de la tabla ordenes\n",
    "    \"\"\"\n",
    "    estado_orden = ordenes['order_status'].unique()\n",
    "    estado_orden = {j:i+1 for i,j in enumerate(estado_orden)}\n",
    "    order_status_df = pd.DataFrame(\n",
    "        {\n",
    "            'order_status_id': estado_orden.values(),\n",
    "            'order_status': estado_orden.keys()\n",
    "        })\n",
    "    ordenes['order_status'] = ordenes['order_status'].apply(lambda x: estado_orden[x])\n",
    "    ordenes.rename(columns={'order_status': 'order_status_id'}, inplace=True)\n",
    "\n",
    "    # Exportamos los csv\n",
    "    ordenes.to_csv('./Dataset_etl/ordenes_corregido.csv', index=False)\n",
    "    order_status_df.to_csv('./Dataset_etl/order_status.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes_estado_orden(ordenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productos // tipo de producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productos_tipo_producto(productos):\n",
    "    \"\"\"\n",
    "    Se agregan product_category_name_id y se crean las tablas normalizadas product_category_name y productos_corregido a partir de la tabla productos\n",
    "    \"\"\"\n",
    "\n",
    "    productos_name = productos['product_category_name'].unique()\n",
    "    productos_name = {j:i+1 for i,j in enumerate(productos_name)}\n",
    "\n",
    "    product_category_name_df = pd.DataFrame(\n",
    "        {\n",
    "            'product_category_name_id': productos_name.values(),\n",
    "            'category_name': productos_name.keys()\n",
    "        })\n",
    "    productos['product_category_name'] = productos['product_category_name'].apply(lambda x: productos_name[x])\n",
    "    productos.rename(columns={'product_category_name': 'product_category_name_id'}, inplace=True)\n",
    "    \n",
    "    # Exportamos a csv\n",
    "    productos.to_csv('./Dataset_etl/productos_corregido.csv', index=False)\n",
    "    product_category_name_df.to_csv('./Dataset_etl/product_category_name.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos_tipo_producto(productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7264182eebd21301844b523446cf0e3816433127ddc469c000b43f70ae0d1fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
